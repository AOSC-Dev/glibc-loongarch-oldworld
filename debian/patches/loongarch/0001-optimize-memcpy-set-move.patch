From 78475ecd906634da74de2c252416b169dad47bcc Mon Sep 17 00:00:00 2001
From: chenzeshuai <chenzeshuai@loongson.cn>
Date: Wed, 3 Feb 2021 10:27:02 +0800
Subject: [PATCH] optimize memcpy/set/move.

Change-Id: I481dade3fe0dedb50e5fdcf45f15faf8d96ce34e
---
 sysdeps/loongarch/lp64/memcpy.S  | 165 +++++++++++++++++++++----------
 sysdeps/loongarch/lp64/memmove.S |  40 --------
 sysdeps/loongarch/lp64/memset.S  |  24 -----
 3 files changed, 114 insertions(+), 115 deletions(-)

diff --git a/sysdeps/loongarch/lp64/memcpy.S b/sysdeps/loongarch/lp64/memcpy.S
index 64a5393cc2..3d0358325c 100644
--- a/sysdeps/loongarch/lp64/memcpy.S
+++ b/sysdeps/loongarch/lp64/memcpy.S
@@ -84,12 +84,53 @@ LEAF(MEMCPY_NAME)
 	move    a5, a1
 	srai.d	a6, a2, 4  		#num/16
 	beqz	a6, less_16bytes        #num<16
-	srai.d	a6, a2, 6  		#num/64
-	bnez	a6, more_64bytes       #num>64
-	srai.d	a6, a2, 5  
-	beqz	a6, less_32bytes	   #num<32
-		
-	ld.d	t0, a1, 0              #32<num<64
+	slti    a6, a2, 137
+	beqz	a6, more_137bytes       #num>137
+	srai.d	a6, a2, 6  
+	beqz	a6, less_64bytes	   #num<64
+
+	srli.d	a0, a0, 3
+	slli.d	a0, a0, 3
+	addi.d	a0, a0,  0x8
+	sub.d	a7, t8,  a0
+	ld.d	t0, a1, 0
+	sub.d	a1,  a1,  a7
+	st.d	t0, t8, 0
+
+	add.d	a7, a7, a2
+	addi.d	a7, a7, -0x20     
+loop_32:
+	ld.d	t0, a1, 0  
+	ld.d	t1, a1, 8
+	ld.d	t2, a1, 16
+	ld.d	t3, a1, 24
+	st.d	t0, a0, 0  
+	st.d	t1, a0, 8
+	st.d	t2, a0, 16
+	st.d	t3, a0, 24
+
+	addi.d	a0,  a0,   0x20
+	addi.d	a1,  a1,   0x20
+	addi.d	a7,  a7,  -0x20
+	blt     zero, a7, loop_32
+	 
+	ld.d	t4, a4, -32 
+	ld.d	t5, a4, -24
+	ld.d	t6, a4, -16
+	ld.d	t7, a4, -8
+	st.d	t4, a3, -32 
+	st.d	t5, a3, -24
+	st.d	t6, a3, -16
+	st.d	t7, a3, -8
+
+	move	v0,  t8
+	jr	ra
+
+less_64bytes:
+	srai.d  a6, a2, 5
+	beqz    a6, less_32bytes
+
+	ld.d	t0, a1, 0         
 	ld.d	t1, a1, 8
 	ld.d	t2, a1, 16
 	ld.d	t3, a1, 24
@@ -106,7 +147,7 @@ LEAF(MEMCPY_NAME)
     st.d	t6, a3, -16
     st.d	t7, a3, -8
 
-	jr  ra
+	jr	ra
 
 less_32bytes:
 	ld.d	t0, a1, 0
@@ -164,67 +205,54 @@ less_2bytes:
 less_1bytes:
 	jr	ra
 
-more_64bytes:
+more_137bytes:
+	li       a6, 64
+	andi     t1, a0, 7
 	srli.d	a0, a0, 3
+	andi     t2, a2, 7
 	slli.d	a0, a0, 3
-	beq 	a0, t8, all_align
+	add.d   t1, t1, t2
+	beqz 	t1, all_align
+	beq     a0, t8, start_over
 	addi.d	a0, a0, 0x8
 	sub.d	a7, t8, a0
 	sub.d	a1, a1, a7
 	add.d	a2, a7, a2
 
 start_unalign_proc:
+	ld.d    t0, a5, 0
+	slli.d  t0, t0, 8
 	pcaddi  t1, 18
-	slli.d  a6, a7, 3
-	add.d   t1, t1, a6
+	slli.d  t2, a7, 3
+	add.d   t1, t1, t2
 	jirl    zero, t1, 0
 
 start_7_unalign:
-	ld.b    t0, a5, 6
-	st.b    t0, t8, 6
+	srli.d  t0, t0, 8
+	st.b    t0, a0, -7
 start_6_unalign:
-	ld.b    t0, a5, 5
-	st.b    t0, t8, 5
+	srli.d  t0, t0, 8
+	st.b    t0, a0, -6
 start_5_unalign:
-	ld.b    t0, a5, 4
-	st.b    t0, t8, 4
+	srli.d  t0, t0, 8
+	st.b    t0, a0, -5
 start_4_unalign:
-	ld.b    t0, a5, 3
-	st.b    t0, t8, 3
+	srli.d  t0, t0, 8
+	st.b    t0, a0, -4
 start_3_unalign:
-	ld.b    t0, a5, 2
-	st.b    t0, t8, 2
+	srli.d  t0, t0, 8
+	st.b    t0, a0, -3
 start_2_unalign:
-	ld.b    t0, a5, 1
-	st.b    t0, t8, 1
+	srli.d  t0, t0, 8
+	st.b    t0, a0, -2
 start_1_unalign:
-	ld.b    t0, a5, 0
-	st.b    t0, t8, 0
+	srli.d  t0, t0, 8
+	st.b    t0, a0, -1
 start_over:
 
 	addi.d	a2, a2, -0x80
 	blt     a2, zero, end_unalign_proc
 
-	srai.d	a6, a2, 11
-	beqz	a6, loop_less
-	
-loop_more:
-	addi.d  a7, zero, 0x2
-loop_in:
-	LDST_1024
-
-	addi.d	a0, a0, 1024
-	addi.d	a1, a1, 1024
-
-	addi.d  a7, a7, -1
-	bnez    a7, loop_in
-	addi.d	a6, a6, -1
-	bnez	a6, loop_more
-
-	ori     t4, zero, 2048
-  	addi.d 	t4, t4, -1 
-  	and 	a2, a2, t4
-	
 loop_less:
 	LD_64(a1, 0)
 	ST_64(a0, 0)
@@ -291,6 +319,12 @@ end_8_16_unalign:
     	st.d    t0, a0, 0
 end_0_8_unalign:
 
+		mod.d   t0, a3, a6
+		srli.d  t1, t0, 3
+		slti    t0, t0, 1
+		add.d   t0, t0, t1
+		blt		zero, t0, end_8_without_cross_cache_line
+
     	andi    a2, a2, 0x7
 		pcaddi  t1, 18
 		slli.d  a2, a2, 3
@@ -324,12 +358,41 @@ end:
 		jr	ra
 
 all_align:
-	addi.d  a0, a0, 0x8
-	addi.d  a1, a1, 0x8
-	ld.d	t0, a5, 0
-	st.d    t0, t8, 0
-	addi.d  a2, a2, -8
-	b 		start_over
+		addi.d 	a2, a2, -0x20 
+
+align_loop_less:
+		ld.d	t0, a1, 0  
+		ld.d	t1, a1, 8
+		ld.d	t2, a1, 16
+		ld.d	t3, a1, 24
+		st.d	t0, a0, 0  
+		st.d	t1, a0, 8
+		st.d	t2, a0, 16
+		st.d	t3, a0, 24
+
+		addi.d	a0,  a0,   0x20
+		addi.d	a1,  a1,   0x20
+		addi.d	a2,  a2,  -0x20
+		blt		zero, a2, align_loop_less
+
+		ld.d	t4, a4, -32 
+		ld.d	t5, a4, -24
+		ld.d	t6, a4, -16
+		ld.d	t7, a4, -8
+		st.d	t4, a3, -32 
+		st.d	t5, a3, -24
+		st.d	t6, a3, -16
+		st.d	t7, a3, -8
+
+		move    v0,	t8
+		jr 	ra
+
+end_8_without_cross_cache_line:
+		ld.d    t0, a4, -8
+		st.d    t0, a3, -8
+
+		move    v0, t8
+		jr	ra
 
 END(MEMCPY_NAME)
 #ifndef ANDROID_CHANGES
diff --git a/sysdeps/loongarch/lp64/memmove.S b/sysdeps/loongarch/lp64/memmove.S
index cd50c48a56..1d439f57bf 100644
--- a/sysdeps/loongarch/lp64/memmove.S
+++ b/sysdeps/loongarch/lp64/memmove.S
@@ -243,26 +243,6 @@ start_over:
 	addi.d	a2, a2, -0x80
 	blt     a2, zero, end_unalign_proc
 
-	srai.d	a6, a2, 11
-	beqz	a6, loop_less
-	
-loop_more:
-	addi.d  a7, zero, 0x2
-loop_in:
-	LDST_1024
-
-	addi.d	a0, a0, 1024
-	addi.d	a1, a1, 1024
-
-	addi.d  a7, a7, -1
-	bnez    a7, loop_in
-	addi.d	a6, a6, -1
-	bnez	a6, loop_more
-
-	ori     t4, zero, 2048
-  	addi.d 	t4, t4, -1 
-  	and 	a2, a2, t4
-	
 loop_less:
 	LD_64(a1, 0)
 	ST_64(a0, 0)
@@ -412,26 +392,6 @@ start_over_back:
 	addi.d  a2, a2, -0x80
 	blt     a2, zero, end_unalign_proc_back
 
-	srai.d  a6, a2, 11
-	beqz    a6, loop_less_back
-
-loop_more_back:
-	addi.d  a7, zero, 0x2
-loop_in_back:
-	LDST_1024_BACK
-
-	addi.d  a3, a3, -1024
-	addi.d  a4, a4, -1024
-
-	addi.d  a7, a7, -1
-	bnez    a7, loop_in_back
-	addi.d  a6, a6, -1
-	bnez    a6, loop_more_back
-
-	ori     t4, zero, 2048
-  	addi.d 	t4, t4, -1 
-  	and 	a2, a2, t4
-
 loop_less_back:
 	LD_64(a4, -64)
 	ST_64(a3, -64)
diff --git a/sysdeps/loongarch/lp64/memset.S b/sysdeps/loongarch/lp64/memset.S
index 6d6ad44e0f..1f66f98f97 100644
--- a/sysdeps/loongarch/lp64/memset.S
+++ b/sysdeps/loongarch/lp64/memset.S
@@ -114,30 +114,6 @@ more_64bytes:
 	addi.d	  a2, a2, -0x80
 	blt       a2, zero, end_unalign_proc
 
-	srai.d	  t8, a2, 12
-	beqz	  t8, loop_less
-
-loop_more:
-	addi.d    t4, zero, 4
-loop_in:
-	ST_128(0)    #1
-	ST_128(128)  #2
-	ST_128(256)  #3
-	ST_128(384)  #4
-	ST_128(512)  #5
-	ST_128(640)  #6
-	ST_128(768)  #7
-	ST_128(896)  #8
-	addi.d    a0, a0, 1024
-	addi.d    t4, t4, -1
-	bnez      t4, loop_in
-	addi.d	  t8, t8, -1
-	bnez	  t8, loop_more
-
-	lu12i.w	  t3, 1
-	addi.d	  t3, t3, -1
-	and	  	  a2, a2, t3
-
 loop_less:
 	ST_128(0)
 	addi.d	a0, a0,  0x80
-- 
2.20.1

