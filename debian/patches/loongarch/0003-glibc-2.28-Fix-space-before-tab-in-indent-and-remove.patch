diff --git a/elf/elf.h b/elf/elf.h
index 190a27fbab..7c5f4b1f07 100644
--- a/elf/elf.h
+++ b/elf/elf.h
@@ -3943,7 +3943,7 @@ enum
 #define R_LARCH_NONE		0
 #define R_LARCH_32		1
 #define R_LARCH_64		2
-#define R_LARCH_RELATIVE		3
+#define R_LARCH_RELATIVE	3
 #define R_LARCH_COPY		4
 #define R_LARCH_JUMP_SLOT	5
 #define R_LARCH_TLS_DTPMOD32	6
diff --git a/sysdeps/loongarch/lp64/memcpy.S b/sysdeps/loongarch/lp64/memcpy.S
index 3d0358325c..25049320e6 100644
--- a/sysdeps/loongarch/lp64/memcpy.S
+++ b/sysdeps/loongarch/lp64/memcpy.S
@@ -20,7 +20,7 @@
 	ld.d	t4, reg, n+32; \
 	ld.d	t5, reg, n+40; \
 	ld.d	t6, reg, n+48; \
-	ld.d	t7, reg, n+56; 
+	ld.d	t7, reg, n+56;
 
 
 #define ST_64(reg, n) \
@@ -31,8 +31,8 @@
 	st.d	t4, reg, n+32; \
 	st.d	t5, reg, n+40; \
 	st.d	t6, reg, n+48; \
-	st.d	t7, reg, n+56; 
-	
+	st.d	t7, reg, n+56;
+
 #define LDST_1024 \
 	LD_64(a1, 0);    \
 	ST_64(a0, 0);    \
@@ -65,7 +65,7 @@
 	LD_64(a1, 896);  \
 	ST_64(a0, 896);  \
 	LD_64(a1, 960);  \
-	ST_64(a0, 960);  
+	ST_64(a0, 960);
 
 #ifdef ANDROID_CHANGES
 LEAF(MEMCPY_NAME, 0)
@@ -86,7 +86,7 @@ LEAF(MEMCPY_NAME)
 	beqz	a6, less_16bytes        #num<16
 	slti    a6, a2, 137
 	beqz	a6, more_137bytes       #num>137
-	srai.d	a6, a2, 6  
+	srai.d	a6, a2, 6
 	beqz	a6, less_64bytes	   #num<64
 
 	srli.d	a0, a0, 3
@@ -98,13 +98,13 @@ LEAF(MEMCPY_NAME)
 	st.d	t0, t8, 0
 
 	add.d	a7, a7, a2
-	addi.d	a7, a7, -0x20     
+	addi.d	a7, a7, -0x20
 loop_32:
-	ld.d	t0, a1, 0  
+	ld.d	t0, a1, 0
 	ld.d	t1, a1, 8
 	ld.d	t2, a1, 16
 	ld.d	t3, a1, 24
-	st.d	t0, a0, 0  
+	st.d	t0, a0, 0
 	st.d	t1, a0, 8
 	st.d	t2, a0, 16
 	st.d	t3, a0, 24
@@ -113,12 +113,12 @@ loop_32:
 	addi.d	a1,  a1,   0x20
 	addi.d	a7,  a7,  -0x20
 	blt     zero, a7, loop_32
-	 
-	ld.d	t4, a4, -32 
+
+	ld.d	t4, a4, -32
 	ld.d	t5, a4, -24
 	ld.d	t6, a4, -16
 	ld.d	t7, a4, -8
-	st.d	t4, a3, -32 
+	st.d	t4, a3, -32
 	st.d	t5, a3, -24
 	st.d	t6, a3, -16
 	st.d	t7, a3, -8
@@ -130,7 +130,7 @@ less_64bytes:
 	srai.d  a6, a2, 5
 	beqz    a6, less_32bytes
 
-	ld.d	t0, a1, 0         
+	ld.d	t0, a1, 0
 	ld.d	t1, a1, 8
 	ld.d	t2, a1, 16
 	ld.d	t3, a1, 24
@@ -138,7 +138,7 @@ less_64bytes:
 	ld.d	t5, a4, -24
 	ld.d	t6, a4, -16
 	ld.d	t7, a4, -8
-    st.d	t0, a0, 0  
+    st.d	t0, a0, 0
     st.d	t1, a0, 8
     st.d	t2, a0, 16
     st.d	t3, a0, 24
@@ -164,7 +164,7 @@ less_32bytes:
 less_16bytes:
 	srai.d	a6, a2, 3 #num/8
 	beqz	a6, less_8bytes
-	
+
 	ld.d	t0, a1, 0
 	ld.d	t1, a4, -8
 	st.d	t0, a0, 0
@@ -182,11 +182,11 @@ less_8bytes:
 	st.w	t1, a3, -4
 
 	jr	ra
-	
+
 less_4bytes:
 	srai.d	a6, a2, 1
 	beqz	a6, less_2bytes
-	
+
 	ld.h	t0, a1, 0
 	ld.h	t1, a4, -2
 	st.h	t0, a0, 0
@@ -358,14 +358,14 @@ end:
 		jr	ra
 
 all_align:
-		addi.d 	a2, a2, -0x20 
+		addi.d 	a2, a2, -0x20
 
 align_loop_less:
-		ld.d	t0, a1, 0  
+		ld.d	t0, a1, 0
 		ld.d	t1, a1, 8
 		ld.d	t2, a1, 16
 		ld.d	t3, a1, 24
-		st.d	t0, a0, 0  
+		st.d	t0, a0, 0
 		st.d	t1, a0, 8
 		st.d	t2, a0, 16
 		st.d	t3, a0, 24
@@ -375,11 +375,11 @@ align_loop_less:
 		addi.d	a2,  a2,  -0x20
 		blt		zero, a2, align_loop_less
 
-		ld.d	t4, a4, -32 
+		ld.d	t4, a4, -32
 		ld.d	t5, a4, -24
 		ld.d	t6, a4, -16
 		ld.d	t7, a4, -8
-		st.d	t4, a3, -32 
+		st.d	t4, a3, -32
 		st.d	t5, a3, -24
 		st.d	t6, a3, -16
 		st.d	t7, a3, -8
diff --git a/sysdeps/loongarch/lp64/memmove.S b/sysdeps/loongarch/lp64/memmove.S
index 1d439f57bf..f87d036bcf 100644
--- a/sysdeps/loongarch/lp64/memmove.S
+++ b/sysdeps/loongarch/lp64/memmove.S
@@ -20,7 +20,7 @@
 	ld.d	t4, reg, n+32; \
 	ld.d	t5, reg, n+40; \
 	ld.d	t6, reg, n+48; \
-	ld.d	t7, reg, n+56; 
+	ld.d	t7, reg, n+56;
 
 
 #define ST_64(reg, n) \
@@ -31,8 +31,8 @@
 	st.d	t4, reg, n+32; \
 	st.d	t5, reg, n+40; \
 	st.d	t6, reg, n+48; \
-	st.d	t7, reg, n+56; 
-	
+	st.d	t7, reg, n+56;
+
 #define LDST_1024 \
 	LD_64(a1, 0);    \
 	ST_64(a0, 0);    \
@@ -65,7 +65,7 @@
 	LD_64(a1, 896);  \
 	ST_64(a0, 896);  \
 	LD_64(a1, 960);  \
-	ST_64(a0, 960);  
+	ST_64(a0, 960);
 
 #define LDST_1024_BACK \
 	LD_64(a4, -64);   \
@@ -99,7 +99,7 @@
 	LD_64(a4, -960);  \
 	ST_64(a3, -960);  \
 	LD_64(a4, -1024); \
-	ST_64(a3, -1024); 
+	ST_64(a3, -1024);
 
 #ifdef ANDROID_CHANGES
 LEAF(MEMMOVE_NAME, 0)
@@ -120,9 +120,9 @@ LEAF(MEMMOVE_NAME)
 	beqz	a6, less_16bytes        #num<16
 	srai.d	a6, a2, 6  		#num/64
 	bnez	a6, more_64bytes       #num>64
-	srai.d	a6, a2, 5  
+	srai.d	a6, a2, 5
 	beqz	a6, less_32bytes	   #num<32
-		
+
 	ld.d	t0, a1, 0              #32<num<64
 	ld.d	t1, a1, 8
 	ld.d	t2, a1, 16
@@ -131,7 +131,7 @@ LEAF(MEMMOVE_NAME)
 	ld.d	t5, a4, -24
 	ld.d	t6, a4, -16
 	ld.d	t7, a4, -8
-    st.d	t0, a0, 0  
+    st.d	t0, a0, 0
     st.d	t1, a0, 8
     st.d	t2, a0, 16
     st.d	t3, a0, 24
@@ -157,7 +157,7 @@ less_32bytes:
 less_16bytes:
 	srai.d	a6, a2, 3 #num/8
 	beqz	a6, less_8bytes
-	
+
 	ld.d	t0, a1, 0
 	ld.d	t1, a4, -8
 	st.d	t0, a0, 0
@@ -175,11 +175,11 @@ less_8bytes:
 	st.w	t1, a3, -4
 
 	jr	ra
-	
+
 less_4bytes:
 	srai.d	a6, a2, 1
 	beqz	a6, less_2bytes
-	
+
 	ld.h	t0, a1, 0
 	ld.h	t1, a4, -2
 	st.h	t0, a0, 0
@@ -367,7 +367,7 @@ copy_backward:
 	sub.d   a7, a3, a5
 	add.d   a4, a4, a7
 	add.d   a2, a7, a2
-	
+
 	pcaddi  t1, 18
 	slli.d  a6, a7, 3
 	add.d   t1, t1, a6
@@ -449,7 +449,7 @@ end_unalign_proc_back:
 		slli.d  a2, a2, 3
 		sub.d   t1, t1, a2
 		jirl    zero, t1, 0
-	
+
 		ld.b    t0, a1, 6
 		st.b    t0, a0, 6
 		ld.b    t0, a1, 5
diff --git a/sysdeps/loongarch/lp64/memset.S b/sysdeps/loongarch/lp64/memset.S
index 1f66f98f97..8bc152ee22 100644
--- a/sysdeps/loongarch/lp64/memset.S
+++ b/sysdeps/loongarch/lp64/memset.S
@@ -32,7 +32,7 @@
 	st.d    a1, a0, n+120;	 \
 
 //1st var: void *str  $4 a0
-//2nd var: int val  $5   a1 
+//2nd var: int val  $5   a1
 //3rd var: size_t num  $6  a2
 
 LEAF(MEMSET)
@@ -67,7 +67,7 @@ less_32bytes:
 	st.d	  a1, a0, 8
 	st.d	  a1, t7, -16
 	st.d	  a1, t7, -8
-	
+
 	jr	  ra
 
 less_16bytes:
@@ -75,7 +75,7 @@ less_16bytes:
 	beqz	  t8, less_8bytes
 	st.d	  a1, a0, 0
 	st.d	  a1, t7, -8
-	
+
 	jr	  ra
 
 less_8bytes:
@@ -97,7 +97,7 @@ less_4bytes:
 less_2bytes:
 	beqz	  a2, less_1bytes
 	st.b	  a1, a0, 0
-	
+
 	jr	  ra
 
 less_1bytes:
@@ -110,7 +110,7 @@ more_64bytes:
 	st.d      a1, t0, 0
 	sub.d	  t2, t0, a0
 	add.d	  a2, t2, a2
-	
+
 	addi.d	  a2, a2, -0x80
 	blt       a2, zero, end_unalign_proc
 
diff --git a/sysdeps/loongarch/lp64/strchr.S b/sysdeps/loongarch/lp64/strchr.S
index 57cd2ec7c8..a3bf46419c 100644
--- a/sysdeps/loongarch/lp64/strchr.S
+++ b/sysdeps/loongarch/lp64/strchr.S
@@ -60,8 +60,8 @@ LEAF(STRCHR)
 	slli.w		t1, t0, 3
 
 	ld.d		t4, t4, 0
-	
-	
+
+
 	nor		t8, zero, zero
 	bstrins.d	a1, a1, 31, 16
 	srl.d		t4, t4, t1
diff --git a/sysdeps/loongarch/lp64/strchrnul.S b/sysdeps/loongarch/lp64/strchrnul.S
index b58fedd6c7..dfa4788a85 100644
--- a/sysdeps/loongarch/lp64/strchrnul.S
+++ b/sysdeps/loongarch/lp64/strchrnul.S
@@ -69,8 +69,8 @@ LEAF(STRCHRNUL)
 	ldr		t4, 0(a0)
 */
 	ld.d		t4, t4, 0
-	
-	
+
+
 	nor		t8, zero, zero
 	bstrins.d	a1, a1, 31, 16
 	srl.d		t4, t4, t1
@@ -99,7 +99,7 @@ LEAF(STRCHRNUL)
 	or          a7, a6, a5
 	bnez		a7, L(_mc8_a)
 
-	
+
     sub.w		t1, t1, t0
 	L_ADDU		a0, a0, t1
 L(_aloop):
@@ -113,7 +113,7 @@ L(_aloop):
 	nor		a4, t2, a3
 	and		a6, a7, a6
 	and		a5, a5, a4
-	
+
     or          a7, a6, a5
 	bnez		a7, L(_mc8_a)
 
@@ -128,7 +128,7 @@ L(_aloop):
 	nor		    a4, t2, a3
 	and		    a6, a7, a6
 	and		    a5, a5, a4
-	
+
     or          a7, a6, a5
 	beqz		a7, L(_aloop)
 
diff --git a/sysdeps/loongarch/lp64/strcmp.S b/sysdeps/loongarch/lp64/strcmp.S
index 3d653e8412..11474bf2d0 100644
--- a/sysdeps/loongarch/lp64/strcmp.S
+++ b/sysdeps/loongarch/lp64/strcmp.S
@@ -64,7 +64,7 @@
 #define src2_off    a3
 #define tmp4        a7
 
-/* rd <- if rc then ra else rb 
+/* rd <- if rc then ra else rb
     will destroy tmp3
 */
 #define CONDITIONSEL(rd,rc,ra,rb)\
@@ -112,7 +112,7 @@ strcmp_end:
 	andi		data2, data2, 0xff
 	sub.d		result, data1, data2
 	jr ra
-strcmp_mutual_align:	
+strcmp_mutual_align:
     bstrins.d   src1, zero, 2, 0
     bstrins.d   src2, zero, 2, 0
 	slli.d		tmp1, src1_off,  0x3
@@ -129,7 +129,7 @@ strcmp_mutual_align:
 
 strcmp_misaligned8:
 
-/* check 
+/* check
     if ((src1 != 0) && ((src2 == 0 ) || (src1 < src2)))
     then exchange(src1,src2)
 
diff --git a/sysdeps/loongarch/lp64/strcpy.S b/sysdeps/loongarch/lp64/strcpy.S
index f18238a9f9..2c30471e52 100644
--- a/sysdeps/loongarch/lp64/strcpy.S
+++ b/sysdeps/loongarch/lp64/strcpy.S
@@ -8,9 +8,9 @@
  */
 
 /* basic algorithm :
-    
-    +.  if src aligned. just do the copy loop. if not, do the cross page check and copy one double word. 
-    
+
+    +.  if src aligned. just do the copy loop. if not, do the cross page check and copy one double word.
+
         Then move src to aligned.
 
 	+.	if (v0 - 0x0101010101010101) & (~v0) & 0x8080808080808080 != 0, v0 has
@@ -55,7 +55,7 @@
 #define src_off     a3
 #define tmp4        a7
 
-/* rd <- if rc then ra else rb 
+/* rd <- if rc then ra else rb
     will destroy tmp3
 */
 #define CONDITIONSEL(rd,rc,ra,rb)\
@@ -94,13 +94,13 @@ strcpy_start_realigned:
 
 strcpy_end:
 
-/* 
+/*
 8 4 2 1
 */
 	ctz.d		pos, has_nul
 	srli.d		pos, pos, 3
     addi.d      pos, pos, 1
-/* 
+/*
     Do 8/4/2/1 strcpy based on pos value.
     pos value is the number of bytes to be copied
     the bytes include the final \0 so the max length is 8 and the min length is 1
@@ -133,8 +133,8 @@ strcpy_end_ret:
     jr  ra
 
 
-strcpy_mutual_align:	
-/*  
+strcpy_mutual_align:
+/*
     Check if around src page bound.
     if not go to page cross ok.
     if it is, do further check.
@@ -146,10 +146,10 @@ strcpy_mutual_align:
     beq         tmp1, tmp2, strcpy_page_cross
 
 strcpy_page_cross_ok:
-/* 
-    Load a misaligned double word and check if has \0 
+/*
+    Load a misaligned double word and check if has \0
     If no, do a misaligned double word paste.
-    If yes, calculate the number of avaliable bytes, 
+    If yes, calculate the number of avaliable bytes,
     then jump to 4/2/1 end.
 */
     ld.d        data, src, 0
@@ -158,7 +158,7 @@ strcpy_page_cross_ok:
 	andn		has_nul, tmp1, tmp2
     bnez        has_nul, strcpy_end
 strcpy_mutual_align_finish:
-/* 
+/*
     Before jump back to align loop, make dest/src aligned.
     This will cause a duplicated paste for several bytes between the first double word and the second double word,
     but should not bring a problem.
@@ -172,25 +172,25 @@ strcpy_mutual_align_finish:
 	b		strcpy_loop_aligned_1
 
 strcpy_page_cross:
-/*   
+/*
     ld.d from aligned address(src & ~0x7).
     check if high bytes have \0.
-    it not, go back to page cross ok, 
+    it not, go back to page cross ok,
     since the string is supposed to cross the page bound in such situation.
     if it is, do a srl for data to make it seems like a direct double word from src,
     then go to 4/2/1 strcpy end.
 
-    tmp4 is 0xffff...ffff mask 
+    tmp4 is 0xffff...ffff mask
     tmp2 demonstrate the bytes to be masked
     tmp2 = src_off << 3
     data = data >> (src_off * 8) | -1 << (64 - src_off * 8)
     and
     -1 << (64 - src_off * 8) ->  ~(-1 >> (src_off * 8))
-                                
+
 */
     li          tmp1, 0x7
     andn        tmp3, src,  tmp1
-    ld.d        data, tmp3, 0	
+    ld.d        data, tmp3, 0
     li          tmp4, -1
     slli.d      tmp2, src_off, 3
     srl.d       tmp4, tmp4, tmp2
diff --git a/sysdeps/loongarch/lp64/strlen.S b/sysdeps/loongarch/lp64/strlen.S
index 6075dd0068..74b01a103c 100644
--- a/sysdeps/loongarch/lp64/strlen.S
+++ b/sysdeps/loongarch/lp64/strlen.S
@@ -46,7 +46,7 @@ algorithm:
 	cfi_startproc ;
 	.type	strlen, @function;
 strlen:
-    
+
     //LEAF(strlen)
     #preld       0, a0, 0
 
diff --git a/sysdeps/loongarch/lp64/strncmp.S b/sysdeps/loongarch/lp64/strncmp.S
index 03b368db43..365dcb2c31 100644
--- a/sysdeps/loongarch/lp64/strncmp.S
+++ b/sysdeps/loongarch/lp64/strncmp.S
@@ -128,7 +128,7 @@ strncmp_not_limit:
 
 
 
-strncmp_mutual_align:	
+strncmp_mutual_align:
     bstrins.d   src1, zero, 2, 0
     bstrins.d   src2, zero, 2, 0
 	slli.d		tmp1, src1_off,  0x3
@@ -188,7 +188,7 @@ strncmp_try_words:
     andi        src1_off, src1_off, 0x7
     sub.d       limit, limit, src1_off
     srli.d      limit_wd, limit, 0x3
-    
+
 
 strncmp_page_end_loop:
     ld.bu       data1, src1, 0
@@ -250,7 +250,7 @@ strncmp_done_loop:
 strncmp_ret0:
     move result, zero
     jr ra
-/* check 
+/* check
     if ((src1 != 0) && ((src2 == 0 ) || (src1 < src2)))
     then exchange(src1,src2)
 
diff --git a/sysdeps/loongarch/lp64/strnlen.S b/sysdeps/loongarch/lp64/strnlen.S
index 9232c03fbb..6ee875f224 100644
--- a/sysdeps/loongarch/lp64/strnlen.S
+++ b/sysdeps/loongarch/lp64/strnlen.S
@@ -35,7 +35,7 @@ algorithm:
 
 #define STRNLEN	__strnlen
 #define L(x)	x
-/* rd <- if rc then ra else rb 
+/* rd <- if rc then ra else rb
     will destroy t6
 */
 
@@ -118,7 +118,7 @@ L(_nul_in_data2):
     CONDITIONSEL(len,len,limit,tmp1)
     jr ra
 
-    
+
 L(misaligned):
     addi.d      limit_wd, limit, -1
     sub.d       tmp4, zero, tmp1
@@ -144,7 +144,7 @@ L(misaligned):
 
 L(_hit_limit):
     move len, limit
-    jr  ra 
+    jr  ra
 END(STRNLEN)
 #ifndef ANDROID_CHANGES
 #ifdef _LIBC
diff --git a/sysdeps/unix/sysv/linux/loongarch/getcontext.S b/sysdeps/unix/sysv/linux/loongarch/getcontext.S
index 675a9bd17e..89f25f1216 100644
--- a/sysdeps/unix/sysv/linux/loongarch/getcontext.S
+++ b/sysdeps/unix/sysv/linux/loongarch/getcontext.S
@@ -63,7 +63,7 @@ LEAF (__getcontext)
 	syscall 0
 	blt	a0, zero, 99f
 
-	jirl    $r0, $r1, 0	
+	jirl    $r0, $r1, 0
 
 99:	b	__syscall_error
 
diff --git a/sysdeps/unix/sysv/linux/loongarch/vfork.S b/sysdeps/unix/sysv/linux/loongarch/vfork.S
index 42e6d7996a..73c8e66683 100644
--- a/sysdeps/unix/sysv/linux/loongarch/vfork.S
+++ b/sysdeps/unix/sysv/linux/loongarch/vfork.S
@@ -1,49 +1,49 @@
 /* Copyright (C) 1999-2018 Free Software Foundation, Inc.
-          
+
    This file is part of the GNU C Library.
-          
+
    The GNU C Library is free software; you can redistribute it and/or
    modify it under the terms of the GNU Lesser General Public License as
    published by the Free Software Foundation; either version 2.1 of the
    License, or (at your option) any later version.
-          
+
    The GNU C Library is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
    Lesser General Public License for more details.
-          
+
    You should have received a copy of the GNU Lesser General Public
    License along with the GNU C Library; if not, see
    <http://www.gnu.org/licenses/>.  */
-          
+
 #include <sysdep.h>
 #define _ERRNO_H    1
 #include <bits/errno.h>
-          
+
 /* Clone the calling process, but without copying the whole address space.
    The calling process is suspended until the new process exits or is
    replaced by a call to `execve'.  Return -1 for errors, 0 to the new process,
    and the process ID of the new process to the old process.  */
-          
+
 ENTRY (__vfork)
 
-    
+
     dli a0, 0x4111 /* CLONE_VM | CLONE_VFORK | SIGCHLD */
     add.d a1, zero, sp
 
     /* Do the system call.  */
     dli a7, __NR_clone
     syscall 0
-          
+
     blt a0, zero ,L (error)
 
-    ret   
-          
+    ret
+
 L (error):
     b   __syscall_error
-    END (__vfork)    
+    END (__vfork)
 
 libc_hidden_def (__vfork)
-          
+
 weak_alias (__vfork, vfork)
 strong_alias (__vfork, __libc_vfork)
-- 
2.20.1

