From 36fd24db95de676d9008eba934609205223a9371 Mon Sep 17 00:00:00 2001
From: chenglulu <chenglulu@loongson.cn>
Date: Tue, 22 Dec 2020 11:16:00 +0800
Subject: [PATCH] Optimized MEM function of 20201217.

Change-Id: If619123881d10ea2039d048ea367d496efbf96e1
---
 sysdeps/loongarch/lp64/memcpy.S  | 449 ++++++++++++---------------
 sysdeps/loongarch/lp64/memmove.S | 516 +++++++++++++++++++++++++++++++
 sysdeps/loongarch/lp64/memset.S  | 333 ++++++--------------
 3 files changed, 804 insertions(+), 494 deletions(-)
 create mode 100644 sysdeps/loongarch/lp64/memmove.S

diff --git a/sysdeps/loongarch/lp64/memcpy.S b/sysdeps/loongarch/lp64/memcpy.S
index 0add236f4e..64a5393cc2 100644
--- a/sysdeps/loongarch/lp64/memcpy.S
+++ b/sysdeps/loongarch/lp64/memcpy.S
@@ -33,6 +33,39 @@
 	st.d	t6, reg, n+48; \
 	st.d	t7, reg, n+56; 
 	
+#define LDST_1024 \
+	LD_64(a1, 0);    \
+	ST_64(a0, 0);    \
+	LD_64(a1, 64);   \
+	ST_64(a0, 64);   \
+	LD_64(a1, 128);  \
+	ST_64(a0, 128);  \
+	LD_64(a1, 192);  \
+	ST_64(a0, 192);  \
+	LD_64(a1, 256);  \
+	ST_64(a0, 256);  \
+	LD_64(a1, 320);  \
+	ST_64(a0, 320);  \
+	LD_64(a1, 384);  \
+	ST_64(a0, 384);  \
+	LD_64(a1, 448);  \
+	ST_64(a0, 448);  \
+	LD_64(a1, 512);  \
+	ST_64(a0, 512);  \
+	LD_64(a1, 576);  \
+	ST_64(a0, 576);  \
+	LD_64(a1, 640);  \
+	ST_64(a0, 640);  \
+	LD_64(a1, 704);  \
+	ST_64(a0, 704);  \
+	LD_64(a1, 768);  \
+	ST_64(a0, 768);  \
+	LD_64(a1, 832);  \
+	ST_64(a0, 832);  \
+	LD_64(a1, 896);  \
+	ST_64(a0, 896);  \
+	LD_64(a1, 960);  \
+	ST_64(a0, 960);  
 
 #ifdef ANDROID_CHANGES
 LEAF(MEMCPY_NAME, 0)
@@ -48,40 +81,15 @@ LEAF(MEMCPY_NAME)
 	add.d	a4, a1, a2
 	add.d	a3, a0, a2
 	move	t8, a0
+	move    a5, a1
 	srai.d	a6, a2, 4  		#num/16
 	beqz	a6, less_16bytes        #num<16
-	srai.d	a6, a2, 8  		#num/256
-	bnez	a6, more_256bytes       #num>256
-	srai.d	a6, a2, 7  
-	beqz	a6, less_128bytes	#num<128
-
-	LD_64(a1, 0)			#128<num<256
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
-	LD_64(a4, -128)
-	ST_64(a3, -128)
-	LD_64(a4, -64)
-	ST_64(a3, -64)
-
-	jr	ra
-
-less_128bytes:
-	srai.d	a6, a2, 6  #num/64
-	beqz	a6, less_64bytes
-
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a4, -64)
-	ST_64(a3, -64)
-	
-	jr	ra
-
-less_64bytes:
-	srai.d	a6, a2, 5 #num/32
-	beqz	a6, less_32bytes
-	
-	ld.d	t0, a1, 0
+	srai.d	a6, a2, 6  		#num/64
+	bnez	a6, more_64bytes       #num>64
+	srai.d	a6, a2, 5  
+	beqz	a6, less_32bytes	   #num<32
+		
+	ld.d	t0, a1, 0              #32<num<64
 	ld.d	t1, a1, 8
 	ld.d	t2, a1, 16
 	ld.d	t3, a1, 24
@@ -89,16 +97,16 @@ less_64bytes:
 	ld.d	t5, a4, -24
 	ld.d	t6, a4, -16
 	ld.d	t7, a4, -8
-	st.d	t0, a0, 0
-	st.d	t1, a0, 8
-	st.d	t2, a0, 16
-	st.d	t3, a0, 24
-	st.d	t4, a3, -32
-	st.d	t5, a3, -24
-	st.d	t6, a3, -16
-	st.d	t7, a3, -8
+    st.d	t0, a0, 0  
+    st.d	t1, a0, 8
+    st.d	t2, a0, 16
+    st.d	t3, a0, 24
+    st.d	t4, a3, -32
+    st.d	t5, a3, -24
+    st.d	t6, a3, -16
+    st.d	t7, a3, -8
 
-	jr	ra
+	jr  ra
 
 less_32bytes:
 	ld.d	t0, a1, 0
@@ -156,231 +164,172 @@ less_2bytes:
 less_1bytes:
 	jr	ra
 
-more_256bytes:
+more_64bytes:
 	srli.d	a0, a0, 3
 	slli.d	a0, a0, 3
-	addi.d	a0, a0,  0x8
-	sub.d	a7, t8,  a0
-	ld.d	t0, a1, 0
-	sub.d	a1,  a1,  a7
-	st.d	t0, t8, 0
-
-	LD_64(a4, -128)
-	ST_64(a3, -128)
-	LD_64(a4, -64)
-	ST_64(a3, -64)
-
-	add.d	a7, a7, a2
-	addi.d	a7, a7, -0x80     
-	srai.d	a6, a7, 22    
-	bnez	a6, loop_most
-	srai.d	a6, a7, 12
+	beq 	a0, t8, all_align
+	addi.d	a0, a0, 0x8
+	sub.d	a7, t8, a0
+	sub.d	a1, a1, a7
+	add.d	a2, a7, a2
+
+start_unalign_proc:
+	pcaddi  t1, 18
+	slli.d  a6, a7, 3
+	add.d   t1, t1, a6
+	jirl    zero, t1, 0
+
+start_7_unalign:
+	ld.b    t0, a5, 6
+	st.b    t0, t8, 6
+start_6_unalign:
+	ld.b    t0, a5, 5
+	st.b    t0, t8, 5
+start_5_unalign:
+	ld.b    t0, a5, 4
+	st.b    t0, t8, 4
+start_4_unalign:
+	ld.b    t0, a5, 3
+	st.b    t0, t8, 3
+start_3_unalign:
+	ld.b    t0, a5, 2
+	st.b    t0, t8, 2
+start_2_unalign:
+	ld.b    t0, a5, 1
+	st.b    t0, t8, 1
+start_1_unalign:
+	ld.b    t0, a5, 0
+	st.b    t0, t8, 0
+start_over:
+
+	addi.d	a2, a2, -0x80
+	blt     a2, zero, end_unalign_proc
+
+	srai.d	a6, a2, 11
 	beqz	a6, loop_less
-
+	
 loop_more:
-	addi.d	a5, zero, 4
-loop_in_1:
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
-
-	addi.d	a0, a0, 128	#1
-	addi.d	a1, a1, 128	#1
-
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
-
-	addi.d	a0, a0, 128	#2
-	addi.d	a1, a1, 128	#2
-
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
-
-	addi.d	a0, a0, 128	#3
-	addi.d	a1, a1, 128	#3
-
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
-
-	addi.d	a0, a0, 128	#4
-	addi.d	a1, a1, 128	#4
-
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
-
-	addi.d	a0, a0, 128	#5
-	addi.d	a1, a1, 128	#5
-
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
-
-	addi.d	a0, a0, 128	#6
-	addi.d	a1, a1, 128	#6
-
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
+	addi.d  a7, zero, 0x2
+loop_in:
+	LDST_1024
 
-	addi.d	a0, a0, 128	#7
-	addi.d	a1, a1, 128	#7
+	addi.d	a0, a0, 1024
+	addi.d	a1, a1, 1024
 
-	LD_64(a1, 0)
-	ST_64(a0, 0)
-	LD_64(a1, 64)
-	ST_64(a0, 64)
-
-	addi.d	a0, a0, 128	#8
-	addi.d	a1, a1, 128	#8
-
-	addi.d	a5, a5, -1
-	bnez	a5, loop_in_1
+	addi.d  a7, a7, -1
+	bnez    a7, loop_in
 	addi.d	a6, a6, -1
 	bnez	a6, loop_more
 
- 	lu12i.w t4, 1
+	ori     t4, zero, 2048
   	addi.d 	t4, t4, -1 
-  	and 	a7, a7, t4
+  	and 	a2, a2, t4
 	
-	b	loop_less
-
-loop_most:
-	srai.d	a6, a7, 12
-	lu12i.w	a3, 1
-	add.d	a2, a3, a0
-	add.d	a3, a3, a1
-
-loop_most_loop:
-	addi.d	a5, zero, 4
-loop_in_2:
-	LD_64(a1, 0)
-	preld	0, a3, 0
-	ST_64(a0, 0)
-	preld	8, a2, 0
-	LD_64(a1, 64)
-	preld	0, a3, 64
-	ST_64(a0, 64)
-	preld	8, a2, 64
-	addi.d	a0,  a0,   128   #1
-	addi.d	a1,  a1,   128   #1
-	addi.d  a2,  a2,   128
-   	addi.d  a3,  a3,   128
-	LD_64(a1, 0)
-	preld	0, a3, 0
-	ST_64(a0, 0)
-	preld	8, a2, 0
-	LD_64(a1, 64)
-	preld	0, a3, 64
-	ST_64(a0, 64)
-	preld	8, a2, 64
-	addi.d	a0,  a0,   128   #2
-	addi.d	a1,  a1,   128   #2
-	addi.d  a2,  a2,   128
-   	addi.d  a3,  a3,   128
-	LD_64(a1, 0)
-	preld	0, a3, 0
-	ST_64(a0, 0)
-	preld	8, a2, 0
-	LD_64(a1, 64)
-	preld	0, a3, 64
-	ST_64(a0, 64)
-	preld	8, a2, 64
-	addi.d	a0,  a0,   128   #3
-	addi.d	a1,  a1,   128   #3
-	addi.d  a2,  a2,   128
-   	addi.d  a3,  a3,   128
-	LD_64(a1, 0)
-	preld	0, a3, 0
-	ST_64(a0, 0)
-	preld	8, a2, 0
-	LD_64(a1, 64)
-	preld	0, a3, 64
-	ST_64(a0, 64)
-	preld	8, a2, 64
-	addi.d	a0,  a0,   128   #4
-	addi.d	a1,  a1,   128   #4
-	addi.d  a2,  a2,   128
-   	addi.d  a3,  a3,   128
-	LD_64(a1, 0)
-	preld	0, a3, 0
-	ST_64(a0, 0)
-	preld	8, a2, 0
-	LD_64(a1, 64)
-	preld	0, a3, 64
-	ST_64(a0, 64)
-	preld	8, a2, 64
-	addi.d	a0,  a0,   128   #5
-	addi.d	a1,  a1,   128   #5
-	addi.d  a2,  a2,   128
-   	addi.d  a3,  a3,   128
-	LD_64(a1, 0)
-	preld	0, a3, 0
-	ST_64(a0, 0)
-	preld	8, a2, 0
-	LD_64(a1, 64)
-	preld	0, a3, 64
-	ST_64(a0, 64)
-	preld	8, a2, 64
-	addi.d	a0,  a0,   128   #6
-	addi.d	a1,  a1,   128   #6
-	addi.d  a2,  a2,   128
-   	addi.d  a3,  a3,   128
-	LD_64(a1, 0)
-	preld	0, a3, 0
-	ST_64(a0, 0)
-	preld	8, a2, 0
-	LD_64(a1, 64)
-	preld	0, a3, 64
-	ST_64(a0, 64)
-	preld	8, a2, 64
-	addi.d	a0,  a0,   128   #7
-	addi.d	a1,  a1,   128   #7
-	addi.d  a2,  a2,   128
-   	addi.d  a3,  a3,   128
-	LD_64(a1, 0)
-	preld	0, a3, 0
-	ST_64(a0, 0)
-	preld	8, a2, 0
-	LD_64(a1, 64)
-	preld	0, a3, 64
-	ST_64(a0, 64)
-	preld	8, a2, 64
-	addi.d	a0,  a0,   128   #8
-	addi.d	a1,  a1,   128   #8
-	addi.d  a2,  a2,   128
-   	addi.d  a3,  a3,   128
-	addi.d	a5, a5, -1
-	bnez	a5, loop_in_2
-	addi.d	a6, a6, -1
-	bnez	a6, loop_most_loop
-
- 	lu12i.w t4, 1
-  	addi.d 	t4, t4, -1 
-  	and 	a7, a7, t4
-
 loop_less:
 	LD_64(a1, 0)
 	ST_64(a0, 0)
 	LD_64(a1, 64)
 	ST_64(a0, 64)
-	addi.d	a0,  a0,   0x80
-	addi.d	a1,  a1,   0x80
-	addi.d	a7,  a7, -0x80
-	slt	a6, a7, zero
-	beqz	a6, loop_less
-	move	v0, t8
-	jr	ra
+
+	addi.d	a0, a0,  0x80
+	addi.d	a1, a1,  0x80
+	addi.d	a2, a2, -0x80
+	bge     a2, zero, loop_less
+
+end_unalign_proc:
+		addi.d  a2, a2, 0x80
+
+    	pcaddi  t1, 34
+    	andi    t2, a2, 0x78
+    	sub.d   t1, t1, t2
+    	jirl    zero, t1, 0
+
+end_120_128_unalign:
+		ld.d    t0, a1, 112
+		st.d    t0, a0, 112
+end_112_120_unalign:
+		ld.d    t0, a1, 104
+		st.d    t0, a0, 104
+end_104_112_unalign:
+		ld.d    t0, a1, 96
+		st.d    t0, a0, 96
+end_96_104_unalign:
+		ld.d    t0, a1, 88
+		st.d    t0, a0, 88
+end_88_96_unalign:
+		ld.d    t0, a1, 80
+		st.d    t0, a0, 80
+end_80_88_unalign:
+		ld.d    t0, a1, 72
+		st.d    t0, a0, 72
+end_72_80_unalign:
+		ld.d    t0, a1, 64
+		st.d    t0, a0, 64
+end_64_72_unalign:
+		ld.d    t0, a1, 56
+		st.d    t0, a0, 56
+end_56_64_unalign:
+		ld.d    t0, a1, 48
+		st.d    t0, a0, 48
+end_48_56_unalign:
+		ld.d    t0, a1, 40
+		st.d    t0, a0, 40
+end_40_48_unalign:
+		ld.d    t0, a1, 32
+		st.d    t0, a0, 32
+end_32_40_unalign:
+		ld.d    t0, a1, 24
+		st.d    t0, a0, 24
+end_24_32_unalign:
+    	ld.d    t0, a1, 16
+    	st.d    t0, a0, 16
+end_16_24_unalign:
+    	ld.d    t0, a1, 8
+    	st.d    t0, a0, 8
+end_8_16_unalign:
+    	ld.d    t0, a1, 0
+    	st.d    t0, a0, 0
+end_0_8_unalign:
+
+    	andi    a2, a2, 0x7
+		pcaddi  t1, 18
+		slli.d  a2, a2, 3
+		sub.d   t1, t1, a2
+		jirl    zero, t1, 0
+
+end_7_unalign:
+		ld.b    t0, a4, -7
+		st.b    t0, a3, -7
+end_6_unalign:
+		ld.b    t0, a4, -6
+		st.b    t0, a3, -6
+end_5_unalign:
+		ld.b    t0, a4, -5
+		st.b    t0, a3, -5
+end_4_unalign:
+		ld.b    t0, a4, -4
+		st.b    t0, a3, -4
+end_3_unalign:
+		ld.b    t0, a4, -3
+		st.b    t0, a3, -3
+end_2_unalign:
+		ld.b    t0, a4, -2
+		st.b    t0, a3, -2
+end_1_unalign:
+		ld.b    t0, a4, -1
+		st.b    t0, a3, -1
+end:
+
+		move    v0, t8
+		jr	ra
+
+all_align:
+	addi.d  a0, a0, 0x8
+	addi.d  a1, a1, 0x8
+	ld.d	t0, a5, 0
+	st.d    t0, t8, 0
+	addi.d  a2, a2, -8
+	b 		start_over
 
 END(MEMCPY_NAME)
 #ifndef ANDROID_CHANGES
diff --git a/sysdeps/loongarch/lp64/memmove.S b/sysdeps/loongarch/lp64/memmove.S
new file mode 100644
index 0000000000..cd50c48a56
--- /dev/null
+++ b/sysdeps/loongarch/lp64/memmove.S
@@ -0,0 +1,516 @@
+#ifdef _LIBC
+#include <sysdep.h>
+#include <sys/regdef.h>
+#include <sys/asm.h>
+#else
+#include <regdef.h>
+#include <sys/asm.h>
+#endif
+
+/* Allow the routine to be named something else if desired.  */
+#ifndef MEMMOVE_NAME
+#define MEMMOVE_NAME memmove
+#endif
+
+#define LD_64(reg, n) \
+	ld.d	t0, reg, n;    \
+	ld.d	t1, reg, n+8;  \
+	ld.d	t2, reg, n+16; \
+	ld.d	t3, reg, n+24; \
+	ld.d	t4, reg, n+32; \
+	ld.d	t5, reg, n+40; \
+	ld.d	t6, reg, n+48; \
+	ld.d	t7, reg, n+56; 
+
+
+#define ST_64(reg, n) \
+	st.d	t0, reg, n;    \
+	st.d	t1, reg, n+8;  \
+	st.d	t2, reg, n+16; \
+	st.d	t3, reg, n+24; \
+	st.d	t4, reg, n+32; \
+	st.d	t5, reg, n+40; \
+	st.d	t6, reg, n+48; \
+	st.d	t7, reg, n+56; 
+	
+#define LDST_1024 \
+	LD_64(a1, 0);    \
+	ST_64(a0, 0);    \
+	LD_64(a1, 64);   \
+	ST_64(a0, 64);   \
+	LD_64(a1, 128);  \
+	ST_64(a0, 128);  \
+	LD_64(a1, 192);  \
+	ST_64(a0, 192);  \
+	LD_64(a1, 256);  \
+	ST_64(a0, 256);  \
+	LD_64(a1, 320);  \
+	ST_64(a0, 320);  \
+	LD_64(a1, 384);  \
+	ST_64(a0, 384);  \
+	LD_64(a1, 448);  \
+	ST_64(a0, 448);  \
+	LD_64(a1, 512);  \
+	ST_64(a0, 512);  \
+	LD_64(a1, 576);  \
+	ST_64(a0, 576);  \
+	LD_64(a1, 640);  \
+	ST_64(a0, 640);  \
+	LD_64(a1, 704);  \
+	ST_64(a0, 704);  \
+	LD_64(a1, 768);  \
+	ST_64(a0, 768);  \
+	LD_64(a1, 832);  \
+	ST_64(a0, 832);  \
+	LD_64(a1, 896);  \
+	ST_64(a0, 896);  \
+	LD_64(a1, 960);  \
+	ST_64(a0, 960);  
+
+#define LDST_1024_BACK \
+	LD_64(a4, -64);   \
+	ST_64(a3, -64);   \
+	LD_64(a4, -128);  \
+	ST_64(a3, -128);  \
+	LD_64(a4, -192);  \
+	ST_64(a3, -192);  \
+	LD_64(a4, -256);  \
+	ST_64(a3, -256);  \
+	LD_64(a4, -320);  \
+	ST_64(a3, -320);  \
+	LD_64(a4, -384);  \
+	ST_64(a3, -384);  \
+	LD_64(a4, -448);  \
+	ST_64(a3, -448);  \
+	LD_64(a4, -512);  \
+	ST_64(a3, -512);  \
+	LD_64(a4, -576);  \
+	ST_64(a3, -576);  \
+	LD_64(a4, -640);  \
+	ST_64(a3, -640);  \
+	LD_64(a4, -704);  \
+	ST_64(a3, -704);  \
+	LD_64(a4, -768);  \
+	ST_64(a3, -768);  \
+	LD_64(a4, -832);  \
+	ST_64(a3, -832);  \
+	LD_64(a4, -896);  \
+	ST_64(a3, -896);  \
+	LD_64(a4, -960);  \
+	ST_64(a3, -960);  \
+	LD_64(a4, -1024); \
+	ST_64(a3, -1024); 
+
+#ifdef ANDROID_CHANGES
+LEAF(MEMMOVE_NAME, 0)
+#else
+LEAF(MEMMOVE_NAME)
+#endif
+
+//1st var: dest ptr: void *str1 $r4 a0
+//2nd var: src  ptr: void *str2 $r5 a1
+//3rd var: size_t num
+//t0~t9 registers as temp
+
+	add.d	a4, a1, a2
+	add.d	a3, a0, a2
+	beq		a1, a0, less_1bytes
+	move	t8, a0
+	srai.d	a6, a2, 4  		#num/16
+	beqz	a6, less_16bytes        #num<16
+	srai.d	a6, a2, 6  		#num/64
+	bnez	a6, more_64bytes       #num>64
+	srai.d	a6, a2, 5  
+	beqz	a6, less_32bytes	   #num<32
+		
+	ld.d	t0, a1, 0              #32<num<64
+	ld.d	t1, a1, 8
+	ld.d	t2, a1, 16
+	ld.d	t3, a1, 24
+	ld.d	t4, a4, -32
+	ld.d	t5, a4, -24
+	ld.d	t6, a4, -16
+	ld.d	t7, a4, -8
+    st.d	t0, a0, 0  
+    st.d	t1, a0, 8
+    st.d	t2, a0, 16
+    st.d	t3, a0, 24
+    st.d	t4, a3, -32
+    st.d	t5, a3, -24
+    st.d	t6, a3, -16
+    st.d	t7, a3, -8
+
+	jr  ra
+
+less_32bytes:
+	ld.d	t0, a1, 0
+	ld.d	t1, a1, 8
+	ld.d	t2, a4, -16
+	ld.d	t3, a4, -8
+	st.d	t0, a0, 0
+	st.d	t1, a0, 8
+	st.d	t2, a3, -16
+	st.d	t3, a3, -8
+
+	jr	ra
+
+less_16bytes:
+	srai.d	a6, a2, 3 #num/8
+	beqz	a6, less_8bytes
+	
+	ld.d	t0, a1, 0
+	ld.d	t1, a4, -8
+	st.d	t0, a0, 0
+	st.d	t1, a3, -8
+
+	jr	ra
+
+less_8bytes:
+	srai.d	a6, a2, 2
+	beqz	a6, less_4bytes
+
+	ld.w	t0, a1, 0
+	ld.w	t1, a4, -4
+	st.w	t0, a0, 0
+	st.w	t1, a3, -4
+
+	jr	ra
+	
+less_4bytes:
+	srai.d	a6, a2, 1
+	beqz	a6, less_2bytes
+	
+	ld.h	t0, a1, 0
+	ld.h	t1, a4, -2
+	st.h	t0, a0, 0
+	st.h	t1, a3, -2
+
+	jr	ra
+
+less_2bytes:
+	beqz	a2, less_1bytes
+
+	ld.b	t0, a1, 0
+	st.b	t0, a0, 0
+
+	jr	ra
+
+less_1bytes:
+	jr	ra
+
+more_64bytes:
+	sub.d   a7, a0, a1
+	bltu	a7, a2, copy_backward
+
+copy_forward:
+	srli.d	a0, a0, 3
+	slli.d	a0, a0, 3
+	beq 	a0, t8, all_align
+	addi.d	a0, a0, 0x8
+	sub.d	a7, t8, a0
+	sub.d	a1, a1, a7
+	add.d	a2, a7, a2
+
+start_unalign_proc:
+	pcaddi  t1, 18
+	slli.d  a6, a7, 3
+	add.d   t1, t1, a6
+	jirl    zero, t1, 0
+
+start_7_unalign:
+	ld.b    t0, a1, -7
+	st.b    t0, a0, -7
+start_6_unalign:
+	ld.b    t0, a1, -6
+	st.b    t0, a0, -6
+start_5_unalign:
+	ld.b    t0, a1, -5
+	st.b    t0, a0, -5
+start_4_unalign:
+	ld.b    t0, a1, -4
+	st.b    t0, a0, -4
+start_3_unalign:
+	ld.b    t0, a1, -3
+	st.b    t0, a0, -3
+start_2_unalign:
+	ld.b    t0, a1, -2
+	st.b    t0, a0, -2
+start_1_unalign:
+	ld.b    t0, a1, -1
+	st.b    t0, a0, -1
+start_over:
+
+	addi.d	a2, a2, -0x80
+	blt     a2, zero, end_unalign_proc
+
+	srai.d	a6, a2, 11
+	beqz	a6, loop_less
+	
+loop_more:
+	addi.d  a7, zero, 0x2
+loop_in:
+	LDST_1024
+
+	addi.d	a0, a0, 1024
+	addi.d	a1, a1, 1024
+
+	addi.d  a7, a7, -1
+	bnez    a7, loop_in
+	addi.d	a6, a6, -1
+	bnez	a6, loop_more
+
+	ori     t4, zero, 2048
+  	addi.d 	t4, t4, -1 
+  	and 	a2, a2, t4
+	
+loop_less:
+	LD_64(a1, 0)
+	ST_64(a0, 0)
+	LD_64(a1, 64)
+	ST_64(a0, 64)
+
+	addi.d	a0, a0,  0x80
+	addi.d	a1, a1,  0x80
+	addi.d	a2, a2, -0x80
+	bge     a2, zero, loop_less
+
+end_unalign_proc:
+		addi.d  a2, a2, 0x80
+
+    	pcaddi  t1, 36
+    	andi    t2, a2, 0x78
+		add.d   a1, a1, t2
+		add.d   a0, a0, t2
+    	sub.d   t1, t1, t2
+    	jirl    zero, t1, 0
+
+end_120_128_unalign:
+		ld.d    t0, a1, -120
+		st.d    t0, a0, -120
+end_112_120_unalign:
+		ld.d    t0, a1, -112
+		st.d    t0, a0, -112
+end_104_112_unalign:
+		ld.d    t0, a1, -104
+		st.d    t0, a0, -104
+end_96_104_unalign:
+		ld.d    t0, a1, -96
+		st.d    t0, a0, -96
+end_88_96_unalign:
+		ld.d    t0, a1, -88
+		st.d    t0, a0, -88
+end_80_88_unalign:
+		ld.d    t0, a1, -80
+		st.d    t0, a0, -80
+end_72_80_unalign:
+		ld.d    t0, a1, -72
+		st.d    t0, a0, -72
+end_64_72_unalign:
+		ld.d    t0, a1, -64
+		st.d    t0, a0, -64
+end_56_64_unalign:
+		ld.d    t0, a1, -56
+		st.d    t0, a0, -56
+end_48_56_unalign:
+		ld.d    t0, a1, -48
+		st.d    t0, a0, -48
+end_40_48_unalign:
+		ld.d    t0, a1, -40
+		st.d    t0, a0, -40
+end_32_40_unalign:
+		ld.d    t0, a1, -32
+		st.d    t0, a0, -32
+end_24_32_unalign:
+    	ld.d    t0, a1, -24
+    	st.d    t0, a0, -24
+end_16_24_unalign:
+    	ld.d    t0, a1, -16
+    	st.d    t0, a0, -16
+end_8_16_unalign:
+    	ld.d    t0, a1, -8
+    	st.d    t0, a0, -8
+end_0_8_unalign:
+
+    	andi    a2, a2, 0x7
+		pcaddi  t1, 18
+		slli.d  a2, a2, 3
+		sub.d   t1, t1, a2
+		jirl    zero, t1, 0
+
+end_7_unalign:
+		ld.b    t0, a4, -7
+		st.b    t0, a3, -7
+end_6_unalign:
+		ld.b    t0, a4, -6
+		st.b    t0, a3, -6
+end_5_unalign:
+		ld.b    t0, a4, -5
+		st.b    t0, a3, -5
+end_4_unalign:
+		ld.b    t0, a4, -4
+		st.b    t0, a3, -4
+end_3_unalign:
+		ld.b    t0, a4, -3
+		st.b    t0, a3, -3
+end_2_unalign:
+		ld.b    t0, a4, -2
+		st.b    t0, a3, -2
+end_1_unalign:
+		ld.b    t0, a4, -1
+		st.b    t0, a3, -1
+end:
+
+		move    v0, t8
+		jr	ra
+
+all_align:
+	addi.d  a1, a1, 0x8
+	addi.d  a0, a0, 0x8
+	ld.d	t0, a1, -8
+	st.d    t0, a0, -8
+	addi.d  a2, a2, -8
+	b 		start_over
+
+all_align_back:
+	addi.d  a4, a4, -0x8
+	addi.d  a3, a3, -0x8
+	ld.d    t0, a4, 0
+	st.d    t0, a3, 0
+	addi.d  a2, a2, -8
+	b       start_over_back
+
+copy_backward:
+	move    a5, a3
+	srli.d  a3, a3, 3
+	slli.d  a3, a3, 3
+	beq     a3, a5, all_align_back
+	sub.d   a7, a3, a5
+	add.d   a4, a4, a7
+	add.d   a2, a7, a2
+	
+	pcaddi  t1, 18
+	slli.d  a6, a7, 3
+	add.d   t1, t1, a6
+	jirl    zero, t1, 0
+
+	ld.b    t0, a4, 6
+	st.b    t0, a3, 6
+	ld.b    t0, a4, 5
+	st.b    t0, a3, 5
+	ld.b    t0, a4, 4
+	st.b    t0, a3, 4
+	ld.b    t0, a4, 3
+	st.b    t0, a3, 3
+	ld.b    t0, a4, 2
+	st.b    t0, a3, 2
+	ld.b    t0, a4, 1
+	st.b    t0, a3, 1
+	ld.b    t0, a4, 0
+	st.b    t0, a3, 0
+start_over_back:
+
+	addi.d  a2, a2, -0x80
+	blt     a2, zero, end_unalign_proc_back
+
+	srai.d  a6, a2, 11
+	beqz    a6, loop_less_back
+
+loop_more_back:
+	addi.d  a7, zero, 0x2
+loop_in_back:
+	LDST_1024_BACK
+
+	addi.d  a3, a3, -1024
+	addi.d  a4, a4, -1024
+
+	addi.d  a7, a7, -1
+	bnez    a7, loop_in_back
+	addi.d  a6, a6, -1
+	bnez    a6, loop_more_back
+
+	ori     t4, zero, 2048
+  	addi.d 	t4, t4, -1 
+  	and 	a2, a2, t4
+
+loop_less_back:
+	LD_64(a4, -64)
+	ST_64(a3, -64)
+	LD_64(a4, -128)
+	ST_64(a3, -128)
+
+	addi.d a4, a4, -0x80
+	addi.d a3, a3, -0x80
+	addi.d a2, a2, -0x80
+	bge    a2, zero, loop_less_back
+
+end_unalign_proc_back:
+		addi.d  a2, a2, 0x80
+
+		pcaddi  t1, 36
+		andi    t2, a2, 0x78
+		sub.d   a4, a4, t2
+		sub.d   a3, a3, t2
+		sub.d   t1, t1, t2
+		jirl    zero, t1, 0
+
+		ld.d    t0, a4, 112
+		st.d    t0, a3, 112
+		ld.d    t0, a4, 104
+		st.d    t0, a3, 104
+		ld.d    t0, a4, 96
+		st.d    t0, a3, 96
+		ld.d    t0, a4, 88
+		st.d    t0, a3, 88
+		ld.d    t0, a4, 80
+		st.d    t0, a3, 80
+		ld.d    t0, a4, 72
+		st.d    t0, a3, 72
+		ld.d    t0, a4, 64
+		st.d    t0, a3, 64
+		ld.d    t0, a4, 56
+		st.d    t0, a3, 56
+		ld.d    t0, a4, 48
+		st.d    t0, a3, 48
+		ld.d    t0, a4, 40
+		st.d    t0, a3, 40
+		ld.d    t0, a4, 32
+		st.d    t0, a3, 32
+    	ld.d    t0, a4, 24
+    	st.d    t0, a3, 24
+    	ld.d    t0, a4, 16
+    	st.d    t0, a3, 16
+    	ld.d    t0, a4, 8
+    	st.d    t0, a3, 8
+		ld.d    t0, a4, 0
+		st.d    t0, a3, 0
+
+		andi    a2, a2, 0x7
+		pcaddi  t1, 18
+		slli.d  a2, a2, 3
+		sub.d   t1, t1, a2
+		jirl    zero, t1, 0
+	
+		ld.b    t0, a1, 6
+		st.b    t0, a0, 6
+		ld.b    t0, a1, 5
+		st.b    t0, a0, 5
+		ld.b    t0, a1, 4
+		st.b    t0, a0, 4
+		ld.b    t0, a1, 3
+		st.b    t0, a0, 3
+		ld.b    t0, a1, 2
+		st.b    t0, a0, 2
+		ld.b    t0, a1, 1
+		st.b    t0, a0, 1
+		ld.b    t0, a1, 0
+		st.b    t0, a0, 0
+
+		move    v0, t8
+		jr	ra
+
+END(MEMMOVE_NAME)
+#ifndef ANDROID_CHANGES
+#ifdef _LIBC
+libc_hidden_builtin_def (MEMMOVE_NAME)
+#endif
+#endif
diff --git a/sysdeps/loongarch/lp64/memset.S b/sysdeps/loongarch/lp64/memset.S
index 639e6774b0..6d6ad44e0f 100644
--- a/sysdeps/loongarch/lp64/memset.S
+++ b/sysdeps/loongarch/lp64/memset.S
@@ -13,6 +13,24 @@
 #define MEMSET	memset
 #endif
 
+#define ST_128(n) 	\
+	st.d	a1, a0, n;		 \
+	st.d    a1, a0, n+8  ; 	 \
+	st.d    a1, a0, n+16 ;   \
+	st.d    a1, a0, n+24 ;   \
+	st.d    a1, a0, n+32 ;   \
+	st.d    a1, a0, n+40 ;   \
+	st.d    a1, a0, n+48 ;   \
+	st.d    a1, a0, n+56 ;   \
+	st.d    a1, a0, n+64 ;   \
+	st.d    a1, a0, n+72 ;   \
+	st.d    a1, a0, n+80 ;   \
+	st.d    a1, a0, n+88 ;   \
+	st.d    a1, a0, n+96 ;   \
+	st.d    a1, a0, n+104;   \
+	st.d    a1, a0, n+112;   \
+	st.d    a1, a0, n+120;	 \
+
 //1st var: void *str  $4 a0
 //2nd var: int val  $5   a1 
 //3rd var: size_t num  $6  a2
@@ -29,71 +47,11 @@ memset:
 	bstrins.d a1, a1, 63, 32
 	srai.d	  t8, a2, 4         	#num/16
 	beqz	  t8, less_16bytes	#num<16
-	srai.d	  t8, a2, 8		#num/256
-	bnez	  t8, more_256bytes	#num>256
-	srai.d	  t8, a2, 7		#num/128
-	beqz	  t8, less_128bytes	#num<128
-	st.d	  a1, a0, 0		#128<num<256
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
-	st.d	  a1, t7, -128
-	st.d	  a1, t7, -120
-	st.d	  a1, t7, -112
-	st.d	  a1, t7, -104
-	st.d	  a1, t7, -96
-	st.d	  a1, t7, -88
-	st.d	  a1, t7, -80
-	st.d	  a1, t7, -72
-	st.d	  a1, t7, -64
-	st.d	  a1, t7, -56
-	st.d	  a1, t7, -48
-	st.d	  a1, t7, -40
-	st.d	  a1, t7, -32
-	st.d	  a1, t7, -24
-	st.d	  a1, t7, -16
-	st.d	  a1, t7, -8
-
-	jr	  ra
-
-less_128bytes:
 	srai.d	  t8, a2, 6		#num/64
-	beqz	  t8, less_64bytes
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, t7, -64
-	st.d	  a1, t7, -56
-	st.d	  a1, t7, -48
-	st.d	  a1, t7, -40
-	st.d	  a1, t7, -32
-	st.d	  a1, t7, -24
-	st.d	  a1, t7, -16
-	st.d	  a1, t7, -8
-	
-	jr	  ra
-
-less_64bytes:
+	bnez	  t8, more_64bytes	#num>64
 	srai.d	  t8, a2, 5		#num/32
-	beqz	  t8, less_32bytes
-	st.d	  a1, a0, 0
+	beqz	  t8, less_32bytes	#num<32
+	st.d	  a1, a0, 0 		#32<num<64
 	st.d	  a1, a0, 8
 	st.d	  a1, a0, 16
 	st.d	  a1, a0, 24
@@ -145,204 +103,91 @@ less_2bytes:
 less_1bytes:
 	jr	  ra
 
-more_256bytes:
+more_64bytes:
 	srli.d	  a0, a0, 3
 	slli.d	  a0, a0, 3
 	addi.d	  a0, a0, 0x8
-	st.d	  a1, t0, 0
+	st.d      a1, t0, 0
 	sub.d	  t2, t0, a0
-	add.d	  t2, t2, a2
-	addi.d	  t2, t2, -0x80
-	srai.d	  t8, t2, 12
+	add.d	  a2, t2, a2
+	
+	addi.d	  a2, a2, -0x80
+	blt       a2, zero, end_unalign_proc
+
+	srai.d	  t8, a2, 12
 	beqz	  t8, loop_less
 
 loop_more:
-	addi.d	  t4, zero, 4
+	addi.d    t4, zero, 4
 loop_in:
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
-	addi.d	  a0, a0, 128		#1
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
-	addi.d	  a0, a0, 128		#2
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
-	addi.d	  a0, a0, 128		#3
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
-	addi.d	  a0, a0, 128		#4
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
-	addi.d	  a0, a0, 128		#5
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
-	addi.d	  a0, a0, 128		#6
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
-	addi.d	  a0, a0, 128		#7
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
-	addi.d	  a0, a0, 128		#8
-	addi.d	  t4, t4, -1
-	bnez	  t4, loop_in
+	ST_128(0)    #1
+	ST_128(128)  #2
+	ST_128(256)  #3
+	ST_128(384)  #4
+	ST_128(512)  #5
+	ST_128(640)  #6
+	ST_128(768)  #7
+	ST_128(896)  #8
+	addi.d    a0, a0, 1024
+	addi.d    t4, t4, -1
+	bnez      t4, loop_in
 	addi.d	  t8, t8, -1
 	bnez	  t8, loop_more
+
 	lu12i.w	  t3, 1
 	addi.d	  t3, t3, -1
-	and	  t2, t2, t3
+	and	  	  a2, a2, t3
 
 loop_less:
-	st.d	  a1, a0, 0
-	st.d	  a1, a0, 8
-	st.d	  a1, a0, 16
-	st.d	  a1, a0, 24
-	st.d	  a1, a0, 32
-	st.d	  a1, a0, 40
-	st.d	  a1, a0, 48
-	st.d	  a1, a0, 56
-	st.d	  a1, a0, 64
-	st.d	  a1, a0, 72
-	st.d	  a1, a0, 80
-	st.d	  a1, a0, 88
-	st.d	  a1, a0, 96
-	st.d	  a1, a0, 104
-	st.d	  a1, a0, 112
-	st.d	  a1, a0, 120
+	ST_128(0)
 	addi.d	a0, a0,  0x80
-	addi.d	t2, t2, -0x80
-	slt	t8, t2, zero
-	beqz	t8, loop_less
-	st.d	  a1, t7, -128
-	st.d	  a1, t7, -120
-	st.d	  a1, t7, -112
-	st.d	  a1, t7, -104
-	st.d	  a1, t7, -96
-	st.d	  a1, t7, -88
-	st.d	  a1, t7, -80
-	st.d	  a1, t7, -72
-	st.d	  a1, t7, -64
-	st.d	  a1, t7, -56
-	st.d	  a1, t7, -48
-	st.d	  a1, t7, -40
-	st.d	  a1, t7, -32
-	st.d	  a1, t7, -24
-	st.d	  a1, t7, -16
-	st.d	  a1, t7, -8
+	addi.d	a2, a2, -0x80
+	bge     a2, zero, loop_less
+
+end_unalign_proc:
+	addi.d  a2, a2, 0x80
+
+	pcaddi  t1, 20
+	andi    t5, a2, 0x78
+	srli.d  t5, t5, 1
+	sub.d   t1, t1, t5
+	jirl    zero, t1, 0
+
+end_120_128_unalign:
+	st.d    a1, a0, 112
+end_112_120_unalign:
+	st.d    a1, a0, 104
+end_104_112_unalign:
+	st.d    a1, a0, 96
+end_96_104_unalign:
+	st.d    a1, a0, 88
+end_88_96_unalign:
+	st.d    a1, a0, 80
+end_80_88_unalign:
+	st.d    a1, a0, 72
+end_72_80_unalign:
+	st.d    a1, a0, 64
+end_64_72_unalign:
+	st.d    a1, a0, 56
+end_56_64_unalign:
+	st.d    a1, a0, 48
+end_48_56_unalign:
+	st.d    a1, a0, 40
+end_40_48_unalign:
+	st.d    a1, a0, 32
+end_32_40_unalign:
+	st.d    a1, a0, 24
+end_24_32_unalign:
+    st.d    a1, a0, 16
+end_16_24_unalign:
+    st.d    a1, a0, 8
+end_8_16_unalign:
+    st.d    a1, a0, 0
+end_0_8_unalign:
+
+	st.d    a1, t7, -8
 
 	move	  v0, t0
-	
 	jr	  ra
 
 END(MEMSET)
-- 
2.27.0

